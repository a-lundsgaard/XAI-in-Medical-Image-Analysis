{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "# make the module available from the src directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataLoaders.PatientDataLoader' from 'c:\\\\Datalogi\\\\Bachelor\\\\XAI-in-Medical-Image-Analysis\\\\src\\\\dataLoaders\\\\PatientDataLoader.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataLoaders.PatientDataLoader import PatientDataProcessor\n",
    "importlib.reload(sys.modules['src.dataLoaders.PatientDataLoader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_folder = 'OAIData21/'\n",
    "data_path = '../datasets/meta_data/' + meta_folder\n",
    "\n",
    "processor = PatientDataProcessor(base_path=data_path)\n",
    "meta_data = processor.load_all_clinical_data(labels=[\"WOMKP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V00AGE</th>\n",
       "      <th>V00WOMKPL</th>\n",
       "      <th>V00WOMKPR</th>\n",
       "      <th>Sex</th>\n",
       "      <th>V01AGE</th>\n",
       "      <th>V01WOMKPL</th>\n",
       "      <th>V01WOMKPR</th>\n",
       "      <th>Sex</th>\n",
       "      <th>V02AGE</th>\n",
       "      <th>V02WOMKPL</th>\n",
       "      <th>...</th>\n",
       "      <th>V09WOMKPR</th>\n",
       "      <th>Sex</th>\n",
       "      <th>V10AGE</th>\n",
       "      <th>V10WOMKPL</th>\n",
       "      <th>V10WOMKPR</th>\n",
       "      <th>Sex</th>\n",
       "      <th>V11AGE</th>\n",
       "      <th>V11WOMKPL</th>\n",
       "      <th>V11WOMKPR</th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9000099</th>\n",
       "      <td>59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000296</th>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000622</th>\n",
       "      <td>71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000798</th>\n",
       "      <td>56</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001104</th>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999365</th>\n",
       "      <td>56</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999510</th>\n",
       "      <td>50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999862</th>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999865</th>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>69.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999878</th>\n",
       "      <td>72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4796 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V00AGE  V00WOMKPL  V00WOMKPR  Sex  V01AGE  V01WOMKPL  V01WOMKPR  Sex  \\\n",
       "ID                                                                              \n",
       "9000099      59        0.0        1.0    0    60.0        0.0        0.0    0   \n",
       "9000296      69        0.0        0.0    0    70.0        0.0        0.0    0   \n",
       "9000622      71        0.0        3.0    1    72.0        0.0        2.0    1   \n",
       "9000798      56        8.0        0.0    0    58.0        8.0        0.0    0   \n",
       "9001104      72        0.0        4.0    1    73.0        0.0        1.0    1   \n",
       "...         ...        ...        ...  ...     ...        ...        ...  ...   \n",
       "9999365      56        4.0        5.0    0    57.0        3.0        5.0    0   \n",
       "9999510      50        3.0        0.0    0    51.0        1.0        0.0    0   \n",
       "9999862      61        0.0        0.0    1    62.0        0.0        0.0    1   \n",
       "9999865      61        0.0        0.0    1    62.0        0.0        0.0    1   \n",
       "9999878      72        1.0        1.0    1    73.0        1.0        0.0    1   \n",
       "\n",
       "         V02AGE  V02WOMKPL  ...  V09WOMKPR  Sex  V10AGE  V10WOMKPL  V10WOMKPR  \\\n",
       "ID                          ...                                                 \n",
       "9000099     NaN        NaN  ...        0.0    0    66.0        4.0        3.0   \n",
       "9000296     NaN        NaN  ...        0.0    0    77.0        0.0        0.0   \n",
       "9000622     NaN        NaN  ...               1     NaN        NaN        NaN   \n",
       "9000798    58.0        7.0  ...        0.0    0    64.0        5.0        0.0   \n",
       "9001104     NaN        NaN  ...               1    80.0        0.0        1.0   \n",
       "...         ...        ...  ...        ...  ...     ...        ...        ...   \n",
       "9999365     NaN        NaN  ...        1.0    0    64.0        4.0        2.0   \n",
       "9999510     NaN        NaN  ...        1.0    0    58.0        3.0        0.0   \n",
       "9999862     NaN        NaN  ...        0.0    1    69.0        0.0        0.0   \n",
       "9999865     NaN        NaN  ...        0.0    1    69.0        3.0        1.0   \n",
       "9999878     NaN        NaN  ...        0.0    1    79.0        1.0        1.0   \n",
       "\n",
       "         Sex  V11AGE  V11WOMKPL  V11WOMKPR  Sex  \n",
       "ID                                               \n",
       "9000099    0    67.0        4.0        2.0    0  \n",
       "9000296    0    78.0        0.0        0.0    0  \n",
       "9000622    1     NaN        NaN        NaN    1  \n",
       "9000798    0    65.0        3.0        0.0    0  \n",
       "9001104    1     NaN        NaN        NaN    1  \n",
       "...      ...     ...        ...        ...  ...  \n",
       "9999365    0    65.0        5.0        6.0    0  \n",
       "9999510    0    59.0        4.0        0.0    0  \n",
       "9999862    1    70.0        0.0        0.0    1  \n",
       "9999865    1    70.0        0.0        0.0    1  \n",
       "9999878    1    80.0        2.0        1.0    1  \n",
       "\n",
       "[4796 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataLoaders.utils.BalancedDataLoader import BalancedBatchSampler\n",
    "importlib.reload(sys.modules['src.dataLoaders.utils.BalancedDataLoader'])\n",
    "\n",
    "from src.dataLoaders.NiftiDataLoader2 import NiftiDataLoader\n",
    "importlib.reload(sys.modules['src.dataLoaders.NiftiDataLoader2'])\n",
    "\n",
    "from src.transformers.SliceTransformer import SliceAggregateTransform\n",
    "importlib.reload(sys.modules['src.transformers.SliceTransformer'])\n",
    "custom_transforms = [SliceAggregateTransform(keys=[\"image\"], slices=9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\dataLoaders\\saved_data_lists\\data_list.pkl does not exist.\n",
      "Data list loaded: False\n",
      "Using custom sampler: False\n",
      "File c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\dataLoaders\\saved_data_lists\\data_list.pkl does not exist.\n",
      "Visits: dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     10\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m NiftiDataLoader(data_dir\u001b[38;5;241m=\u001b[39mdata_dir, \n\u001b[0;32m     11\u001b[0m                               meta_data_loader\u001b[38;5;241m=\u001b[39mprocessor,\n\u001b[0;32m     12\u001b[0m                               batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m                               custom_sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m                               )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# data_loader.load_data(visit_no=visit, subset_size=2058, cache=\"standard\")\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# data_loader.load_data(subset_size=84, cache=\"standard\")\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#data_loader.load_data(subset_size=4400, cache=\"standard\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# data_loader.load_data(subset_size=8875, cache=\"standard\")\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpersistent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\dataLoaders\\NiftiDataLoader2.py:81\u001b[0m, in \u001b[0;36mNiftiDataLoader.load_data\u001b[1;34m(self, subset_size, cache)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing custom sampler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_sampler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# if not isLoaded:\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m#     if visit_nos is None:\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m#         self.meta_data_loader.load_all_visits()\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m#         self.meta_data_loader.load_specific_visits(visit_nos)\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_data_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_data()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transforms()\n",
      "File \u001b[1;32mc:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\dataLoaders\\NiftiDataLoader2.py:186\u001b[0m, in \u001b[0;36mNiftiDataLoader.create_data_list\u001b[1;34m(self, visit_no)\u001b[0m\n\u001b[0;32m    183\u001b[0m all_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(pd\u001b[38;5;241m.\u001b[39mnotna(\u001b[38;5;28mlist\u001b[39m(labels_left_knee\u001b[38;5;241m.\u001b[39mvalues())))\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Check if all labels are not NaN\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_right\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path_right\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    187\u001b[0m     data_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image_path_right, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: labels_right_knee})\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# print(f\"Labels right knee: {labels_right_knee}\")\u001b[39;00m\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = 'niftiShort'\n",
    "data_path = '../datasets/nifti/'\n",
    "data_dir = f'{data_path}{dataset}'\n",
    "\n",
    "\n",
    "\n",
    "data_dir = 'C:/Users/askel/Downloads/NIFTY/NIFTY/'\n",
    "# max = 8876\n",
    "dim = 128\n",
    "data_loader = NiftiDataLoader(data_dir=data_dir, \n",
    "                              meta_data_loader=processor,\n",
    "                              batch_size=64,\n",
    "                              spatial_resize=(dim, dim, dim),\n",
    "                              cache_rate=0.5, \n",
    "                              replace_rate=1, \n",
    "                              custom_transforms=custom_transforms,\n",
    "                              custom_sampler=False,\n",
    "                              )\n",
    "# data_loader.load_data(visit_no=visit, subset_size=2058, cache=\"standard\")\n",
    "# data_loader.load_data(subset_size=84, cache=\"standard\")\n",
    "#data_loader.load_data(subset_size=4400, cache=\"standard\")\n",
    "# data_loader.load_data(subset_size=8875, cache=\"standard\")\n",
    "data_loader.load_data(cache=\"persistent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.models.medical_models.monai_resnet' from 'c:\\\\Datalogi\\\\Bachelor\\\\XAI-in-Medical-Image-Analysis\\\\src\\\\models\\\\medical_models\\\\monai_resnet.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from src.models.baseModels.resnet_3D_monai import MedicalResNetModel\n",
    "# importlib.reload(sys.modules['src.models.baseModels.resnet_3D_monai'])\n",
    "\n",
    "from src.models.medical_models.medical_resnet import MedicalResNetModel\n",
    "importlib.reload(sys.modules['src.models.medical_models.base_medical'])\n",
    "importlib.reload(sys.modules['src.models.medical_models.medical_resnet'])\n",
    "\n",
    "\n",
    "from src.models.medical_models.monai_resnet import MonaiMedicalResNet\n",
    "importlib.reload(sys.modules['src.models.medical_models.monai_resnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader.train_loader.dataset[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader train loader:  5000 5000\n",
      "Image spatial dimensions:  torch.Size([1, 2, 128, 128])\n",
      "Number of input channels:  2\n",
      "gpu:  cuda:0\n",
      "Is cuda available:  True cuda\n",
      "Number of training images: 5000\n",
      "Epoch 1/300, Train Loss: 10.04913304327377, Val Loss: 25.014243364334106, R^2 Score: -1.5880675315856934\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_5000_height_128_epoch_1_val_25.01_r2_25.01.pth\n",
      "Epoch 2/300, Train Loss: 9.954686905584955, Val Loss: 10.726944833993912, R^2 Score: -0.11885786056518555\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_5000_height_128_epoch_2_val_10.73_r2_10.73.pth\n",
      "Epoch 3/300, Train Loss: 9.680166384463725, Val Loss: 9.971701815724373, R^2 Score: -0.04207277297973633\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_5000_height_128_epoch_3_val_9.97_r2_9.97.pth\n",
      "Epoch 4/300, Train Loss: 9.735412371384838, Val Loss: 10.910458475351334, R^2 Score: -0.13433563709259033\n",
      "Epoch 5/300, Train Loss: 9.607781446651567, Val Loss: 13.793176859617233, R^2 Score: -0.4373997449874878\n",
      "Epoch 6/300, Train Loss: 9.576870798987686, Val Loss: 18.65988278388977, R^2 Score: -0.9342186450958252\n",
      "Epoch 7/300, Train Loss: 9.385744354897422, Val Loss: 14.077895402908325, R^2 Score: -0.459700345993042\n",
      "Epoch 8/300, Train Loss: 9.128751520866915, Val Loss: 12.449521720409393, R^2 Score: -0.2946324348449707\n",
      "Epoch 9/300, Train Loss: 9.124635533970189, Val Loss: 11.860585406422615, R^2 Score: -0.23740577697753906\n",
      "Epoch 10/300, Train Loss: 8.984605436533617, Val Loss: 11.108932957053185, R^2 Score: -0.16113591194152832\n",
      "Epoch 11/300, Train Loss: 8.85311808259648, Val Loss: 17.75699084997177, R^2 Score: -0.8418053388595581\n",
      "Epoch 12/300, Train Loss: 8.703645026156918, Val Loss: 11.237459406256676, R^2 Score: -0.17463767528533936\n",
      "Epoch 13/300, Train Loss: 8.502702683494341, Val Loss: 11.634264439344406, R^2 Score: -0.21001160144805908\n",
      "Epoch 14/300, Train Loss: 8.384870489566246, Val Loss: 11.268606171011925, R^2 Score: -0.1784886121749878\n",
      "Epoch 15/300, Train Loss: 8.190168947408011, Val Loss: 14.805236726999283, R^2 Score: -0.5407010316848755\n",
      "Epoch 16/300, Train Loss: 8.008080123619292, Val Loss: 11.903365552425385, R^2 Score: -0.2439577579498291\n",
      "Epoch 17/300, Train Loss: 7.820574026992524, Val Loss: 11.388537615537643, R^2 Score: -0.192338228225708\n",
      "Epoch 18/300, Train Loss: 7.589098734865396, Val Loss: 21.513718485832214, R^2 Score: -1.234039545059204\n",
      "Epoch 19/300, Train Loss: 7.353767263193996, Val Loss: 11.192714273929596, R^2 Score: -0.16922509670257568\n",
      "Epoch 20/300, Train Loss: 7.101240919924553, Val Loss: 11.099686451256275, R^2 Score: -0.16290295124053955\n",
      "Epoch 21/300, Train Loss: 6.749444860154814, Val Loss: 13.780382663011551, R^2 Score: -0.4360020160675049\n",
      "Epoch 22/300, Train Loss: 6.554941654243386, Val Loss: 11.50279326736927, R^2 Score: -0.2057194709777832\n",
      "Epoch 23/300, Train Loss: 6.176119239094672, Val Loss: 13.323533922433853, R^2 Score: -0.3938112258911133\n",
      "Epoch 24/300, Train Loss: 5.915651703797913, Val Loss: 13.248501040041447, R^2 Score: -0.38726353645324707\n",
      "Epoch 25/300, Train Loss: 5.506827359098153, Val Loss: 17.45154118537903, R^2 Score: -0.8215610980987549\n",
      "Epoch 26/300, Train Loss: 5.246470653211404, Val Loss: 12.66828778386116, R^2 Score: -0.3243523836135864\n",
      "Epoch 27/300, Train Loss: 4.924218644892583, Val Loss: 12.758020006120205, R^2 Score: -0.3356442451477051\n",
      "Epoch 28/300, Train Loss: 4.64664680596973, Val Loss: 25.771396100521088, R^2 Score: -1.6849768161773682\n",
      "Epoch 29/300, Train Loss: 4.3539330321314065, Val Loss: 12.637574218213558, R^2 Score: -0.3241746425628662\n",
      "Epoch 30/300, Train Loss: 4.055921215954145, Val Loss: 14.638721123337746, R^2 Score: -0.5321431159973145\n",
      "Epoch 31/300, Train Loss: 3.845791187550548, Val Loss: 18.531016677618027, R^2 Score: -0.9346638917922974\n",
      "Epoch 32/300, Train Loss: 3.5405021571248545, Val Loss: 15.823683559894562, R^2 Score: -0.6505680084228516\n",
      "Epoch 33/300, Train Loss: 3.2465542299035888, Val Loss: 14.042511016130447, R^2 Score: -0.466796875\n",
      "Epoch 34/300, Train Loss: 2.9752531392626143, Val Loss: 68.31969809532166, R^2 Score: -6.073605060577393\n",
      "Epoch 35/300, Train Loss: 2.6985195061690317, Val Loss: 12.655894711613655, R^2 Score: -0.32502245903015137\n",
      "Epoch 36/300, Train Loss: 2.4176712100153184, Val Loss: 20.517172664403915, R^2 Score: -1.1374754905700684\n",
      "Epoch 37/300, Train Loss: 2.2027742883844326, Val Loss: 13.803453296422958, R^2 Score: -0.4411064386367798\n",
      "Epoch 38/300, Train Loss: 2.00333463544868, Val Loss: 22.715230762958527, R^2 Score: -1.3661625385284424\n",
      "Epoch 39/300, Train Loss: 1.7802406253209524, Val Loss: 15.283747881650925, R^2 Score: -0.594882607460022\n",
      "Epoch 40/300, Train Loss: 1.7427046489166431, Val Loss: 29.547087013721466, R^2 Score: -2.081063985824585\n",
      "Epoch 41/300, Train Loss: 1.5672528743771177, Val Loss: 19.1777803003788, R^2 Score: -0.9993788003921509\n",
      "Epoch 42/300, Train Loss: 1.4726590263892743, Val Loss: 19.571696013212204, R^2 Score: -1.045947551727295\n",
      "Epoch 43/300, Train Loss: 1.3714742157356765, Val Loss: 29.553271174430847, R^2 Score: -2.0525999069213867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m      2\u001b[0m fast_model \u001b[38;5;241m=\u001b[39m MedicalResNetModel(\n\u001b[0;32m      3\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[0;32m      4\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#pretrained=False\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# model.load_model('MedicalResNetModel_18_6212_epoch_38.pth')\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mfast_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m fast_model\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\base_medical.py:152\u001b[0m, in \u001b[0;36mMedicalResNetModelBase.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    150\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(images)\n\u001b[0;32m    151\u001b[0m loss: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[1;32m--> 152\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    154\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\_tensor.py:513\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\overrides.py:1621\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1615\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1616\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1617\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[1;32m-> 1621\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\monai\\data\\meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\_tensor.py:1418\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m-> 1418\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[0;32m   1420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a model instance\n",
    "fast_model = MedicalResNetModel(\n",
    "    num_epochs=300,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    data_loader=data_loader,\n",
    "    dropout_rate=0.1,\n",
    "    depth=18\n",
    "    #pretrained=False\n",
    ")\n",
    "\n",
    "# model.load_model('MedicalResNetModel_18_6212_epoch_38.pth')\n",
    "fast_model.train()\n",
    "fast_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader train loader:  221 7095\n",
      "Image spatial dimensions:  torch.Size([23, 9, 384, 384])\n",
      "Number of input channels:  9\n",
      "gpu:  cuda:0\n",
      "Is cuda available:  True cuda\n",
      "Number of training images: 7095\n",
      "Epoch 1/300, Train Loss: 11.309565663877116, Val Loss: 10.085774149213519, R^2 Score: -0.11270451545715332\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_7095_height_384_epoch_1_val_10.09_r2_10.09.pth\n",
      "Epoch 2/300, Train Loss: 10.306701170373287, Val Loss: 9.551078489848546, R^2 Score: -0.0529780387878418\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_7095_height_384_epoch_2_val_9.55_r2_9.55.pth\n",
      "Epoch 3/300, Train Loss: 10.31646461400511, Val Loss: 9.869509901319232, R^2 Score: -0.08843398094177246\n",
      "Epoch 4/300, Train Loss: 10.2841322195476, Val Loss: 9.702601330620903, R^2 Score: -0.07000589370727539\n",
      "Epoch 5/300, Train Loss: 10.245384546426626, Val Loss: 9.583512135914393, R^2 Score: -0.0558696985244751\n",
      "Epoch 6/300, Train Loss: 10.253393295124106, Val Loss: 9.658893465995789, R^2 Score: -0.06503784656524658\n",
      "Epoch 7/300, Train Loss: 10.224749284632066, Val Loss: 9.640607220785958, R^2 Score: -0.06269574165344238\n",
      "Epoch 8/300, Train Loss: 10.43970873129314, Val Loss: 9.640884603772845, R^2 Score: -0.06256306171417236\n",
      "Epoch 9/300, Train Loss: 10.239769649721378, Val Loss: 9.782074706895012, R^2 Score: -0.07834517955780029\n",
      "Epoch 10/300, Train Loss: 10.343508801309232, Val Loss: 9.915244221687317, R^2 Score: -0.09385740756988525\n",
      "Epoch 11/300, Train Loss: 10.37446888215941, Val Loss: 9.708548171179634, R^2 Score: -0.07088339328765869\n",
      "Epoch 12/300, Train Loss: 10.519488186857819, Val Loss: 9.797427518027169, R^2 Score: -0.080710768699646\n",
      "Epoch 13/300, Train Loss: 10.552698153715868, Val Loss: 9.632504037448339, R^2 Score: -0.06594157218933105\n",
      "Epoch 14/300, Train Loss: 10.493068554822136, Val Loss: 9.963314601353236, R^2 Score: -0.09886312484741211\n",
      "Epoch 15/300, Train Loss: 10.495503701775322, Val Loss: 10.111111044883728, R^2 Score: -0.11505627632141113\n",
      "Epoch 16/300, Train Loss: 10.518809243025283, Val Loss: 9.771666986601693, R^2 Score: -0.07568979263305664\n",
      "Epoch 17/300, Train Loss: 10.70074750826909, Val Loss: 9.926738687923976, R^2 Score: -0.09505462646484375\n",
      "Epoch 18/300, Train Loss: 10.538335501338562, Val Loss: 9.575092792510986, R^2 Score: -0.055136799812316895\n",
      "Epoch 19/300, Train Loss: 10.444953321871175, Val Loss: 14.042750903538295, R^2 Score: -0.5490214824676514\n",
      "Epoch 20/300, Train Loss: 10.413461146850931, Val Loss: 10.128925016948156, R^2 Score: -0.11636018753051758\n",
      "Epoch 21/300, Train Loss: 10.269671721695774, Val Loss: 9.921307836260114, R^2 Score: -0.09526455402374268\n",
      "Epoch 22/300, Train Loss: 10.403243662545044, Val Loss: 10.571058120046343, R^2 Score: -0.1657259464263916\n",
      "Epoch 23/300, Train Loss: 10.55685651895687, Val Loss: 9.851254003388542, R^2 Score: -0.08612525463104248\n",
      "Epoch 24/300, Train Loss: 10.486209177863005, Val Loss: 9.712323239871434, R^2 Score: -0.07161843776702881\n",
      "Epoch 25/300, Train Loss: 10.378254757747392, Val Loss: 10.025189893586296, R^2 Score: -0.10579705238342285\n",
      "Epoch 26/300, Train Loss: 10.3169334948872, Val Loss: 9.409341369356428, R^2 Score: -0.03708982467651367\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_7095_height_384_epoch_26_val_9.41_r2_9.41.pth\n",
      "Epoch 27/300, Train Loss: 10.386623887454762, Val Loss: 9.732465658869062, R^2 Score: -0.07304203510284424\n",
      "Epoch 28/300, Train Loss: 10.393941750893227, Val Loss: 9.792541997773307, R^2 Score: -0.07994723320007324\n"
     ]
    }
   ],
   "source": [
    "# Create a model instance\n",
    "sample_model = MedicalResNetModel(\n",
    "    num_epochs=300,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    data_loader=data_loader,\n",
    "    dropout_rate=0.1,\n",
    "    depth=18\n",
    "    #pretrained=False\n",
    ")\n",
    "\n",
    "# model.load_model('MedicalResNetModel_18_6212_epoch_38.pth')\n",
    "sample_model.train()\n",
    "sample_model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader train loader:  7095 7095\n",
      "Image spatial dimensions:  torch.Size([1, 9, 384, 384])\n",
      "Number of input channels:  9\n",
      "gpu:  cuda:0\n",
      "Is cuda available:  True cuda\n",
      "Number of training images: 7095\n",
      "Epoch 1/300, Train Loss: 9.705845940101197, Val Loss: 12.794479012489319, R^2 Score: -0.407146692276001\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_7095_height_384_epoch_1_val_12.79_r2_12.79.pth\n",
      "Epoch 2/300, Train Loss: 9.556465054849742, Val Loss: 9.323674772466932, R^2 Score: -0.04019498825073242\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_7095_height_384_epoch_2_val_9.32_r2_9.32.pth\n",
      "Epoch 3/300, Train Loss: 9.487849866506783, Val Loss: 43.972400529044016, R^2 Score: -3.8746895790100098\n",
      "Epoch 4/300, Train Loss: 9.430768612284643, Val Loss: 20289.463762555803, R^2 Score: -2253.12646484375\n",
      "Epoch 5/300, Train Loss: 9.471651260660623, Val Loss: 12.042496681213379, R^2 Score: -0.33963727951049805\n",
      "Epoch 6/300, Train Loss: 9.44388519412878, Val Loss: 13.81101495879037, R^2 Score: -0.5352323055267334\n",
      "Epoch 7/300, Train Loss: 9.346254065545331, Val Loss: 15.26611590385437, R^2 Score: -0.6928689479827881\n",
      "Epoch 8/300, Train Loss: 9.151780996788798, Val Loss: 10.73857366187232, R^2 Score: -0.19560039043426514\n",
      "Epoch 9/300, Train Loss: 8.969590544585305, Val Loss: 13.508912563323975, R^2 Score: -0.4993337392807007\n",
      "Epoch 10/300, Train Loss: 8.974606688461852, Val Loss: 11.331732894693102, R^2 Score: -0.26172804832458496\n",
      "Epoch 11/300, Train Loss: 8.720736922680205, Val Loss: 16.53058229173933, R^2 Score: -0.8317948579788208\n",
      "Epoch 12/300, Train Loss: 8.438947143198545, Val Loss: 12.125087908336095, R^2 Score: -0.34806108474731445\n",
      "Epoch 13/300, Train Loss: 8.272783043003573, Val Loss: 25.666559968675887, R^2 Score: -1.848520278930664\n",
      "Epoch 14/300, Train Loss: 8.140041714615695, Val Loss: 40.19330051967076, R^2 Score: -3.4585347175598145\n",
      "Epoch 15/300, Train Loss: 8.0057715614118, Val Loss: 30.89072115080697, R^2 Score: -2.424769401550293\n",
      "Epoch 16/300, Train Loss: 7.893148059546612, Val Loss: 12.62853411265782, R^2 Score: -0.4067411422729492\n",
      "Epoch 17/300, Train Loss: 7.7965332209113525, Val Loss: 31.331973995481217, R^2 Score: -2.4763870239257812\n",
      "Epoch 18/300, Train Loss: 7.684538244481257, Val Loss: 11.495501041412354, R^2 Score: -0.2798762321472168\n",
      "Epoch 19/300, Train Loss: 7.564587341010184, Val Loss: 316.359243120466, R^2 Score: -34.056968688964844\n",
      "Epoch 20/300, Train Loss: 7.453809894203558, Val Loss: 10.713697944368635, R^2 Score: -0.19483590126037598\n",
      "Epoch 21/300, Train Loss: 7.344543969979518, Val Loss: 443.06493868146623, R^2 Score: -48.12590789794922\n",
      "Epoch 22/300, Train Loss: 7.1279174866954795, Val Loss: 21.354702949523926, R^2 Score: -1.3742821216583252\n",
      "Epoch 23/300, Train Loss: 6.9365328780347, Val Loss: 21.027984074183873, R^2 Score: -1.3367838859558105\n",
      "Epoch 24/300, Train Loss: 6.852311257346484, Val Loss: 48.43770851407732, R^2 Score: -4.3729939460754395\n",
      "Epoch 25/300, Train Loss: 6.630512006306619, Val Loss: 54.6462938444955, R^2 Score: -5.063928604125977\n",
      "Epoch 26/300, Train Loss: 6.467605277046583, Val Loss: 23.862097024917603, R^2 Score: -1.6528394222259521\n",
      "Epoch 27/300, Train Loss: 6.131226112142466, Val Loss: 11.922237064157214, R^2 Score: -0.33055639266967773\n",
      "Epoch 28/300, Train Loss: 5.9155005192343175, Val Loss: 16.211479084832327, R^2 Score: -0.7995167970657349\n",
      "Epoch 29/300, Train Loss: 5.592689406658806, Val Loss: 30.238765171595983, R^2 Score: -2.3568637371063232\n",
      "Epoch 30/300, Train Loss: 5.299559523361481, Val Loss: 26.826058319636754, R^2 Score: -1.9694969654083252\n",
      "Epoch 31/300, Train Loss: 4.943916207946352, Val Loss: 50.46293061120169, R^2 Score: -4.600961208343506\n",
      "Epoch 32/300, Train Loss: 4.651874500787143, Val Loss: 16.64198797089713, R^2 Score: -0.8532301187515259\n",
      "Epoch 33/300, Train Loss: 4.428705315715517, Val Loss: 401.3102030072893, R^2 Score: -43.46250915527344\n",
      "Epoch 34/300, Train Loss: 4.059355098938327, Val Loss: 20.77813502720424, R^2 Score: -1.3109757900238037\n",
      "Epoch 35/300, Train Loss: 3.8054522263055084, Val Loss: 10.84100263459342, R^2 Score: -0.21033978462219238\n",
      "Epoch 36/300, Train Loss: 3.6026805647724336, Val Loss: 18.552498272487096, R^2 Score: -1.0667750835418701\n",
      "Epoch 37/300, Train Loss: 3.2777035758708895, Val Loss: 15.05287732396807, R^2 Score: -0.6757336854934692\n",
      "Epoch 38/300, Train Loss: 3.1916883616156557, Val Loss: 26.50370168685913, R^2 Score: -1.947840690612793\n",
      "Epoch 39/300, Train Loss: 2.9749102873633464, Val Loss: 12.26405245065689, R^2 Score: -0.3696920871734619\n",
      "Epoch 40/300, Train Loss: 2.807246957823701, Val Loss: 14.498362592288426, R^2 Score: -0.6156662702560425\n",
      "Epoch 41/300, Train Loss: 2.6905484412429765, Val Loss: 484.6021821158273, R^2 Score: -52.81885528564453\n",
      "Epoch 42/300, Train Loss: 2.5354255976423383, Val Loss: 86.97196960449219, R^2 Score: -8.694470405578613\n",
      "Epoch 43/300, Train Loss: 2.3976789399275225, Val Loss: 211.4918488093785, R^2 Score: -22.466461181640625\n",
      "Epoch 44/300, Train Loss: 2.224178109299253, Val Loss: 20.38577958515712, R^2 Score: -1.2710864543914795\n",
      "Epoch 45/300, Train Loss: 2.208264920243259, Val Loss: 174.5861941746303, R^2 Score: -18.38628387451172\n",
      "Epoch 46/300, Train Loss: 2.0425200469055205, Val Loss: 18.2452381168093, R^2 Score: -1.036212682723999\n",
      "Epoch 47/300, Train Loss: 2.0467471071540015, Val Loss: 20.494267225265503, R^2 Score: -1.2850360870361328\n",
      "Epoch 48/300, Train Loss: 1.9120556170245229, Val Loss: 88.41077341352191, R^2 Score: -8.815557479858398\n",
      "Epoch 49/300, Train Loss: 1.8401424825940316, Val Loss: 44.701622349875315, R^2 Score: -3.962108612060547\n",
      "Epoch 50/300, Train Loss: 1.7487917256032064, Val Loss: 25.621945313044957, R^2 Score: -1.8560206890106201\n",
      "Epoch 51/300, Train Loss: 1.630526497125965, Val Loss: 87.12825311933246, R^2 Score: -8.711071968078613\n",
      "Epoch 52/300, Train Loss: 1.618950271318385, Val Loss: 14.614293132509504, R^2 Score: -0.62993323802948\n",
      "Epoch 53/300, Train Loss: 1.5903403215163732, Val Loss: 168.48641654423304, R^2 Score: -17.767913818359375\n",
      "Epoch 54/300, Train Loss: 1.5008571088274265, Val Loss: 43.38749435969761, R^2 Score: -3.845301628112793\n",
      "Epoch 55/300, Train Loss: 1.4856698058675946, Val Loss: 58.874419825417654, R^2 Score: -5.561473846435547\n",
      "Epoch 56/300, Train Loss: 1.4080887726066806, Val Loss: 32.11976759774344, R^2 Score: -2.5861387252807617\n",
      "Epoch 57/300, Train Loss: 1.368614377893512, Val Loss: 29.02095651626587, R^2 Score: -2.232175588607788\n",
      "Epoch 58/300, Train Loss: 1.3594637841420307, Val Loss: 417.7359613691057, R^2 Score: -45.41435241699219\n",
      "Epoch 59/300, Train Loss: 1.3123970806192795, Val Loss: 71.0453440121242, R^2 Score: -6.8989996910095215\n",
      "Epoch 60/300, Train Loss: 1.297483774728177, Val Loss: 31.846858944211686, R^2 Score: -2.5498528480529785\n",
      "Epoch 61/300, Train Loss: 1.2232360978575803, Val Loss: 14.739659581865583, R^2 Score: -0.6436817646026611\n",
      "Epoch 62/300, Train Loss: 1.2138905023174154, Val Loss: 13.277690257344927, R^2 Score: -0.48186421394348145\n",
      "Epoch 63/300, Train Loss: 1.2062907924303452, Val Loss: 32.39214563369751, R^2 Score: -2.6092147827148438\n",
      "Epoch 64/300, Train Loss: 1.2201753824967436, Val Loss: 27.86435317993164, R^2 Score: -2.108375072479248\n",
      "Epoch 65/300, Train Loss: 1.1184130436907964, Val Loss: 61.94125665937151, R^2 Score: -5.900341033935547\n",
      "Epoch 66/300, Train Loss: 1.1215122726412803, Val Loss: 14.11216619185039, R^2 Score: -0.5744813680648804\n",
      "Epoch 67/300, Train Loss: 1.0670764430711763, Val Loss: 20.681332690375193, R^2 Score: -1.303419589996338\n",
      "Epoch 68/300, Train Loss: 1.0553297364358913, Val Loss: 203.06187166486467, R^2 Score: -21.57766342163086\n",
      "Epoch 69/300, Train Loss: 1.0569226793847502, Val Loss: 94.09283365522113, R^2 Score: -9.46134090423584\n",
      "Epoch 70/300, Train Loss: 1.0190771984440439, Val Loss: 134.89946011134558, R^2 Score: -13.992796897888184\n",
      "Epoch 71/300, Train Loss: 0.9960556650709971, Val Loss: 49.59841356958662, R^2 Score: -4.511590003967285\n",
      "Epoch 72/300, Train Loss: 1.0294688354269677, Val Loss: 17.24947444030217, R^2 Score: -0.9220371246337891\n",
      "Epoch 73/300, Train Loss: 0.9529447475295738, Val Loss: 43.84099251883371, R^2 Score: -3.87324857711792\n",
      "Epoch 74/300, Train Loss: 0.9609495319776197, Val Loss: 18.435853396143234, R^2 Score: -1.0537018775939941\n",
      "Epoch 75/300, Train Loss: 0.9440656029341983, Val Loss: 71.43456023080009, R^2 Score: -6.953212738037109\n",
      "Epoch 76/300, Train Loss: 0.901766286905561, Val Loss: 132.74468312944686, R^2 Score: -13.760188102722168\n",
      "Epoch 77/300, Train Loss: 0.8948685170120514, Val Loss: 111.97455433436802, R^2 Score: -11.470355987548828\n",
      "Epoch 78/300, Train Loss: 0.9174759715391642, Val Loss: 29.096580573490687, R^2 Score: -2.233342409133911\n",
      "Epoch 79/300, Train Loss: 0.921707693000854, Val Loss: 18.317465560776846, R^2 Score: -1.041334629058838\n",
      "Epoch 80/300, Train Loss: 0.8990239439477967, Val Loss: 15.351565326963152, R^2 Score: -0.70947265625\n",
      "Epoch 81/300, Train Loss: 0.8610225383746077, Val Loss: 251.9897907802037, R^2 Score: -26.949966430664062\n",
      "Epoch 82/300, Train Loss: 0.8625887064277683, Val Loss: 184.22085217067175, R^2 Score: -19.389400482177734\n",
      "Epoch 83/300, Train Loss: 0.8387950068357546, Val Loss: 27.18775681086949, R^2 Score: -2.0339174270629883\n",
      "Epoch 84/300, Train Loss: 0.8833437710399141, Val Loss: 29.17061608178275, R^2 Score: -2.2494680881500244\n",
      "Epoch 85/300, Train Loss: 0.8228837341213813, Val Loss: 178.90439932686942, R^2 Score: -18.89488410949707\n",
      "Epoch 86/300, Train Loss: 0.8427525567154424, Val Loss: 310.82237352643693, R^2 Score: -33.58217239379883\n",
      "Epoch 87/300, Train Loss: 0.7987535012838283, Val Loss: 49.88695553370884, R^2 Score: -4.565736293792725\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m MedicalResNetModel(\n\u001b[0;32m      3\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[0;32m      4\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#pretrained=False\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# model.load_model('MedicalResNetModel_18_6212_epoch_38.pth')\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\base_medical.py:153\u001b[0m, in \u001b[0;36mMedicalResNetModelBase.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m     loss: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[0;32m    152\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Compute validation loss once per epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\askel\\anaconda3\\envs\\MLA\\Lib\\site-packages\\torch\\optim\\adam.py:508\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[1;32m--> 508\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a model instance\n",
    "model = MedicalResNetModel(\n",
    "    num_epochs=300,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    data_loader=data_loader,\n",
    "    dropout_rate=0.1,\n",
    "    depth=18\n",
    "    #pretrained=False\n",
    ")\n",
    "\n",
    "# model.load_model('MedicalResNetModel_18_6212_epoch_38.pth')\n",
    "model.train()\n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with the best model\n",
    "# model.load_model('MedicalResNetModel_18_487_height_384_epoch_34_val_0.13.pth')\n",
    "# model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
