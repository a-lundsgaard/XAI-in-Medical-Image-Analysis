{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "# make the module available from the src directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataLoaders.PatientDataLoader' from 'c:\\\\Datalogi\\\\Bachelor\\\\XAI-in-Medical-Image-Analysis\\\\src\\\\dataLoaders\\\\PatientDataLoader.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataLoaders.PatientDataLoader import PatientDataProcessor\n",
    "importlib.reload(sys.modules['src.dataLoaders.PatientDataLoader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_folder = 'OAIData21/'\n",
    "data_path = '../datasets/meta_data/' + meta_folder\n",
    "visit = 0\n",
    "\n",
    "processor = PatientDataProcessor(base_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.dataLoaders.NiftiDataLoader2' from 'c:\\\\Datalogi\\\\Bachelor\\\\XAI-in-Medical-Image-Analysis\\\\src\\\\dataLoaders\\\\NiftiDataLoader2.py'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataLoaders.NiftiDataLoader2 import NiftiDataLoader\n",
    "importlib.reload(sys.modules['src.dataLoaders.NiftiDataLoader2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformers.SliceTransformer import SliceAggregateTransform\n",
    "importlib.reload(sys.modules['src.transformers.SliceTransformer'])\n",
    "custom_transforms = [SliceAggregateTransform(keys=[\"image\"], slices=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\dataLoaders\\saved_data_lists\\data_list.pkl does not exist.\n",
      "Data list loaded: False\n",
      "Missing columns for visit V00: {'V00BMI'}\n",
      "Missing columns for visit V01: {'P01BMI'}\n",
      "Missing columns for visit V02: {'V02PASE', 'P01BMI', 'V02BMI'}\n",
      "Missing columns for visit V03: {'P01BMI'}\n",
      "Missing columns for visit V04: {'V04PASE', 'P01BMI', 'V04BMI'}\n",
      "Missing columns for visit V05: {'P01BMI'}\n",
      "Missing columns for visit V06: {'P01BMI'}\n",
      "Missing columns for visit V07: {'V07PASE', 'P01BMI', 'V07BMI'}\n",
      "Missing columns for visit V08: {'P01BMI'}\n",
      "Missing columns for visit V09: {'V09PASE', 'V09BMI', 'P01BMI'}\n",
      "Missing columns for visit V10: {'P01BMI'}\n",
      "Missing columns for visit V11: {'V11BMI', 'P01BMI', 'V11PASE'}\n",
      "File c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\dataLoaders\\saved_data_lists\\data_list.pkl does not exist.\n",
      "Visits: dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
      "Total images detected: 8875\n",
      "Total images: [{'image': 'C:/Users/askel/Downloads/NIFTY/NIFTY/9000798-Left-V00.nii.gz', 'label': 8.0}, {'image': 'C:/Users/askel/Downloads/NIFTY/NIFTY/9000798-Right-V00.nii.gz', 'label': 0.0}, {'image': 'C:/Users/askel/Downloads/NIFTY/NIFTY/9001695-Left-V00.nii.gz', 'label': 0.0}, {'image': 'C:/Users/askel/Downloads/NIFTY/NIFTY/9001695-Right-V00.nii.gz', 'label': 0.0}, {'image': 'C:/Users/askel/Downloads/NIFTY/NIFTY/9001897-Left-V00.nii.gz', 'label': 0.0}]\n",
      "Example train data: {'image': 'C:/Users/askel/Downloads/NIFTY/NIFTY/9082159-Right-V00.nii.gz', 'label': 13.0}\n",
      "Example validation data: {'image': 'C:/Users/askel/Downloads/NIFTY/NIFTY/9230284-Left-V05.nii.gz', 'label': 0.0}\n",
      "Example test data: {'image': 'C:/Users/askel/Downloads/NIFTY/NIFTY/9866291-Left-V03.nii.gz', 'label': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 6212/6212 [27:39<00:00,  3.74it/s] \n",
      "Loading dataset: 100%|██████████| 886/886 [04:02<00:00,  3.66it/s]\n",
      "Loading dataset: 100%|██████████| 1774/1774 [08:39<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = 'niftiShort'\n",
    "data_path = '../datasets/nifti/'\n",
    "data_dir = f'{data_path}{dataset}'\n",
    "\n",
    "data_dir = 'C:/Users/askel/Downloads/NIFTY/NIFTY/'\n",
    "# max = 8876\n",
    "\n",
    "data_loader = NiftiDataLoader(data_dir=data_dir, \n",
    "                              meta_data_loader=processor, \n",
    "                              batch_size=32,\n",
    "                              spatial_size=(128, 128, 128),\n",
    "                              cache_rate=0.5, \n",
    "                              replace_rate=1, \n",
    "                              custom_transforms=custom_transforms,\n",
    "                              )\n",
    "# data_loader.load_data(visit_no=visit, subset_size=2058, cache=\"standard\")\n",
    "# data_loader.load_data(subset_size=84, cache=\"standard\")\n",
    "#data_loader.load_data(subset_size=4400, cache=\"standard\")\n",
    "data_loader.load_data(subset_size=8875, cache=\"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.models.medical_models.monai_resnet' from 'c:\\\\Datalogi\\\\Bachelor\\\\XAI-in-Medical-Image-Analysis\\\\src\\\\models\\\\medical_models\\\\monai_resnet.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from src.models.baseModels.resnet_3D_monai import MedicalResNetModel\n",
    "# importlib.reload(sys.modules['src.models.baseModels.resnet_3D_monai'])\n",
    "\n",
    "from src.models.medical_models.medical_resnet import MedicalResNetModel\n",
    "importlib.reload(sys.modules['src.models.medical_models.base_medical'])\n",
    "importlib.reload(sys.modules['src.models.medical_models.medical_resnet'])\n",
    "\n",
    "\n",
    "from src.models.medical_models.monai_resnet import MonaiMedicalResNet\n",
    "importlib.reload(sys.modules['src.models.medical_models.monai_resnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image spatial dimensions:  2\n",
      "Number of input channels:  3\n",
      "gpu:  cuda:0\n",
      "Is cuda available:  True cuda\n",
      "Number of training images: 6212\n",
      "Epoch 1/300, Train Loss: 10.558229368160932, Val Loss: 10.295302901949201\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_1_val_10.3.pth\n",
      "Epoch 2/300, Train Loss: 9.560164713248229, Val Loss: 9.348519189017159\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_2_val_9.35.pth\n",
      "Epoch 3/300, Train Loss: 9.202139724829259, Val Loss: 10.145180642604828\n",
      "Epoch 4/300, Train Loss: 9.143593451915644, Val Loss: 9.431537534509387\n",
      "Epoch 5/300, Train Loss: 9.236722424091436, Val Loss: 9.831233373710088\n",
      "Epoch 6/300, Train Loss: 9.143579165140787, Val Loss: 9.26208108663559\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_6_val_9.26.pth\n",
      "Epoch 7/300, Train Loss: 9.077685263218024, Val Loss: 9.048206499644689\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_7_val_9.05.pth\n",
      "Epoch 8/300, Train Loss: 8.947876277336707, Val Loss: 8.951538145542145\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_8_val_8.95.pth\n",
      "Epoch 9/300, Train Loss: 8.842510934976431, Val Loss: 9.522036824907575\n",
      "Epoch 10/300, Train Loss: 8.740943294916397, Val Loss: 9.18485027551651\n",
      "Epoch 11/300, Train Loss: 8.58074073424706, Val Loss: 9.041872016021184\n",
      "Epoch 12/300, Train Loss: 8.345699580510457, Val Loss: 8.480651276452202\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_12_val_8.48.pth\n",
      "Epoch 13/300, Train Loss: 8.327433385604467, Val Loss: 8.273002845900399\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_13_val_8.27.pth\n",
      "Epoch 14/300, Train Loss: 8.15989264586033, Val Loss: 8.40534051827022\n",
      "Epoch 15/300, Train Loss: 7.943482560377855, Val Loss: 8.272623334612165\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_15_val_8.27.pth\n",
      "Epoch 16/300, Train Loss: 7.870334344032483, Val Loss: 8.105624692780632\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_16_val_8.11.pth\n",
      "Epoch 17/300, Train Loss: 7.947433774899213, Val Loss: 8.42786694424493\n",
      "Epoch 18/300, Train Loss: 7.768888507745205, Val Loss: 8.040777538503919\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_18_val_8.04.pth\n",
      "Epoch 19/300, Train Loss: 7.6011066913604735, Val Loss: 7.941353355135236\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_19_val_7.94.pth\n",
      "Epoch 20/300, Train Loss: 7.509291234383216, Val Loss: 7.955303302833012\n",
      "Epoch 21/300, Train Loss: 7.3719562909541985, Val Loss: 8.057277296270643\n",
      "Epoch 22/300, Train Loss: 7.240799958889301, Val Loss: 7.7214023641177585\n",
      "Model saved at c:\\Datalogi\\Bachelor\\XAI-in-Medical-Image-Analysis\\src\\models\\medical_models\\saved_models\\MedicalResNetModel_18_6212_epoch_22_val_7.72.pth\n",
      "Epoch 23/300, Train Loss: 7.075711087691478, Val Loss: 7.848764606884548\n",
      "Epoch 24/300, Train Loss: 6.9855024618980215, Val Loss: 8.061945847102574\n",
      "Epoch 25/300, Train Loss: 6.689964426480807, Val Loss: 8.101164145129067\n",
      "Epoch 26/300, Train Loss: 6.385458282935313, Val Loss: 7.899075627326965\n",
      "Epoch 27/300, Train Loss: 5.982129688140674, Val Loss: 8.961612445967537\n",
      "Epoch 28/300, Train Loss: 5.8803946180221365, Val Loss: 7.955786649669919\n",
      "Epoch 29/300, Train Loss: 5.273729988244864, Val Loss: 8.224597547735486\n",
      "Epoch 30/300, Train Loss: 4.932098383781238, Val Loss: 9.182010982717786\n",
      "Epoch 31/300, Train Loss: 4.509534793022351, Val Loss: 8.530758661883217\n",
      "Epoch 32/300, Train Loss: 3.9796802260936834, Val Loss: 10.00447119985308\n",
      "Epoch 33/300, Train Loss: 3.896494560975295, Val Loss: 8.573971424783979\n",
      "Epoch 34/300, Train Loss: 3.435880667124039, Val Loss: 8.811305361134666\n",
      "Epoch 35/300, Train Loss: 3.311470293998718, Val Loss: 8.92359105178288\n",
      "Epoch 36/300, Train Loss: 3.010189640827668, Val Loss: 8.890003681182861\n",
      "Epoch 37/300, Train Loss: 2.903348057697981, Val Loss: 8.789922654628754\n",
      "Epoch 38/300, Train Loss: 2.556113855044047, Val Loss: 8.710382614816938\n",
      "Epoch 39/300, Train Loss: 2.451046083829342, Val Loss: 8.93573408467429\n",
      "Epoch 40/300, Train Loss: 2.24193019591845, Val Loss: 10.301887571811676\n",
      "Epoch 41/300, Train Loss: 2.2758518093671554, Val Loss: 9.765883139201573\n",
      "Epoch 42/300, Train Loss: 2.193893770682506, Val Loss: 8.862734130450658\n",
      "Epoch 43/300, Train Loss: 1.9990808545014798, Val Loss: 8.883608588150569\n",
      "Epoch 44/300, Train Loss: 1.8443400612244238, Val Loss: 9.401177517005376\n",
      "Epoch 45/300, Train Loss: 1.8824185628157395, Val Loss: 8.925790693078723\n",
      "Epoch 46/300, Train Loss: 2.1233014232073075, Val Loss: 8.731638278279986\n",
      "Epoch 47/300, Train Loss: 1.7552192275340741, Val Loss: 8.720606446266174\n",
      "Epoch 48/300, Train Loss: 1.7170088087901092, Val Loss: 8.871963909694127\n",
      "Epoch 49/300, Train Loss: 1.7900285601615906, Val Loss: 8.560022132737297\n",
      "Epoch 50/300, Train Loss: 1.5748282541067173, Val Loss: 8.907475778034755\n",
      "Epoch 51/300, Train Loss: 1.4967721483646295, Val Loss: 8.754643848964147\n",
      "Epoch 52/300, Train Loss: 1.5136848249496557, Val Loss: 8.913115348134722\n",
      "Epoch 53/300, Train Loss: 1.4087370459850017, Val Loss: 8.691716398511614\n",
      "Epoch 54/300, Train Loss: 1.5553911517827939, Val Loss: 9.046468572957176\n",
      "Epoch 55/300, Train Loss: 1.577431606176572, Val Loss: 8.522609506334577\n",
      "Epoch 56/300, Train Loss: 1.533559803167979, Val Loss: 8.977797593389239\n",
      "Epoch 57/300, Train Loss: 1.3811028359792172, Val Loss: 8.323686386857714\n",
      "Epoch 58/300, Train Loss: 1.331503880635286, Val Loss: 8.564816406794957\n",
      "Epoch 59/300, Train Loss: 1.245957566377444, Val Loss: 8.987359353474208\n",
      "Epoch 60/300, Train Loss: 1.3560373237499823, Val Loss: 8.45559847354889\n",
      "Epoch 61/300, Train Loss: 1.23270708888005, Val Loss: 8.625581281525749\n",
      "Epoch 62/300, Train Loss: 1.5029799051773853, Val Loss: 8.661360153130122\n",
      "Epoch 63/300, Train Loss: 1.4250142102058116, Val Loss: 9.24249770811626\n",
      "Epoch 64/300, Train Loss: 1.1532218896425688, Val Loss: 8.668648992265974\n",
      "Epoch 65/300, Train Loss: 1.0966238663746761, Val Loss: 8.91085764339992\n",
      "Epoch 66/300, Train Loss: 1.092074568455036, Val Loss: 9.136418683188301\n",
      "Epoch 67/300, Train Loss: 1.0952670539036775, Val Loss: 9.151524126529694\n",
      "Epoch 68/300, Train Loss: 1.2473766216864952, Val Loss: 8.485092375959669\n",
      "Epoch 69/300, Train Loss: 1.3198223528189537, Val Loss: 8.641362522329603\n",
      "Epoch 70/300, Train Loss: 1.2535708354069637, Val Loss: 8.978761732578278\n",
      "Epoch 71/300, Train Loss: 1.2120489594263908, Val Loss: 8.932068390505654\n",
      "Epoch 72/300, Train Loss: 1.4052466438366817, Val Loss: 9.337304881640843\n",
      "Epoch 73/300, Train Loss: 1.1436624728716336, Val Loss: 8.826512234551567\n",
      "Epoch 74/300, Train Loss: 1.062392325202624, Val Loss: 8.855683411870684\n",
      "Epoch 75/300, Train Loss: 1.1150169219726171, Val Loss: 8.49023813860757\n",
      "Epoch 76/300, Train Loss: 0.9990368467875016, Val Loss: 9.068731078079768\n",
      "Epoch 77/300, Train Loss: 1.0156021286279728, Val Loss: 9.04332572221756\n",
      "Epoch 78/300, Train Loss: 0.9755578151116004, Val Loss: 9.892707245690483\n",
      "Epoch 79/300, Train Loss: 1.0905222163750574, Val Loss: 9.055836864880153\n",
      "Epoch 80/300, Train Loss: 1.0176595075008197, Val Loss: 11.214797343526568\n",
      "Epoch 81/300, Train Loss: 1.2749821178424052, Val Loss: 9.137208768299647\n",
      "Epoch 82/300, Train Loss: 1.1462833867623257, Val Loss: 8.344717281205314\n",
      "Epoch 83/300, Train Loss: 1.0296710783854508, Val Loss: 9.713706578527178\n",
      "Epoch 84/300, Train Loss: 0.9475321951584939, Val Loss: 8.777336282389504\n",
      "Epoch 85/300, Train Loss: 0.990250313282013, Val Loss: 8.61200565951211\n",
      "Epoch 86/300, Train Loss: 0.9906819036373725, Val Loss: 9.354247195380074\n",
      "Epoch 87/300, Train Loss: 1.0431631794342628, Val Loss: 8.97515652009419\n",
      "Epoch 88/300, Train Loss: 0.9830161999433469, Val Loss: 9.086956569126674\n",
      "Epoch 89/300, Train Loss: 0.9693157850931853, Val Loss: 8.601179250649043\n",
      "Epoch 90/300, Train Loss: 1.0184690431142465, Val Loss: 9.231722022805895\n",
      "Epoch 91/300, Train Loss: 0.9355093653385456, Val Loss: 8.97962372643607\n",
      "Epoch 92/300, Train Loss: 1.0328434265576876, Val Loss: 9.025278764111656\n",
      "Epoch 93/300, Train Loss: 1.2152070164680482, Val Loss: 8.956306883266993\n",
      "Epoch 94/300, Train Loss: 1.1554566866312272, Val Loss: 8.521220913955144\n",
      "Epoch 95/300, Train Loss: 0.9774596788944342, Val Loss: 8.833964705467224\n",
      "Epoch 96/300, Train Loss: 0.8615641387609335, Val Loss: 8.913497081824712\n",
      "Epoch 97/300, Train Loss: 0.8057209234971267, Val Loss: 8.751428774424962\n",
      "Epoch 98/300, Train Loss: 1.6130459019770989, Val Loss: 8.966006951672691\n",
      "Epoch 99/300, Train Loss: 1.191303668725185, Val Loss: 9.142426235335213\n",
      "Epoch 100/300, Train Loss: 0.943032735433334, Val Loss: 8.621175161429814\n",
      "Epoch 101/300, Train Loss: 0.907378136423918, Val Loss: 8.61028925861631\n",
      "Epoch 102/300, Train Loss: 0.753335399658252, Val Loss: 8.74765430177961\n",
      "Epoch 103/300, Train Loss: 0.8173213190757311, Val Loss: 8.760574987956456\n",
      "Epoch 104/300, Train Loss: 0.8715599180032045, Val Loss: 8.72384534563337\n",
      "Epoch 105/300, Train Loss: 0.7367836726017487, Val Loss: 8.872208161013466\n",
      "Epoch 106/300, Train Loss: 0.801364999398207, Val Loss: 9.31402598108564\n",
      "Epoch 107/300, Train Loss: 0.8487030884394279, Val Loss: 8.627970252718244\n",
      "Epoch 108/300, Train Loss: 1.1023424096596546, Val Loss: 9.165117289338793\n",
      "Epoch 109/300, Train Loss: 1.0602960387865703, Val Loss: 8.471381723880768\n",
      "Epoch 110/300, Train Loss: 1.023962072837047, Val Loss: 8.63863844530923\n",
      "Epoch 111/300, Train Loss: 0.9555332123469084, Val Loss: 8.632726780005864\n",
      "Epoch 112/300, Train Loss: 0.7771910160779953, Val Loss: 8.692199741091047\n",
      "Epoch 113/300, Train Loss: 0.8185009900576029, Val Loss: 8.536305214677538\n",
      "Epoch 114/300, Train Loss: 0.7681876191726098, Val Loss: 8.840957999229431\n",
      "Epoch 115/300, Train Loss: 0.7748554008893478, Val Loss: 9.032428928783961\n",
      "Epoch 116/300, Train Loss: 0.8293674838848603, Val Loss: 8.636148273944855\n",
      "Epoch 117/300, Train Loss: 1.215562871633432, Val Loss: 8.888680228165217\n",
      "Epoch 118/300, Train Loss: 0.9437486599653195, Val Loss: 9.446669970239912\n",
      "Epoch 119/300, Train Loss: 0.8643713462047088, Val Loss: 8.57028579711914\n",
      "Epoch 120/300, Train Loss: 0.7632131062256984, Val Loss: 8.914550576891218\n",
      "Epoch 121/300, Train Loss: 0.805022957538947, Val Loss: 8.373766311577388\n",
      "Epoch 122/300, Train Loss: 0.7153841798886275, Val Loss: 8.803159270967756\n",
      "Epoch 123/300, Train Loss: 0.7361681415484502, Val Loss: 9.0613226549966\n",
      "Epoch 124/300, Train Loss: 1.5021492548477955, Val Loss: 9.343706488609314\n",
      "Epoch 125/300, Train Loss: 1.046780605155688, Val Loss: 9.231116822787694\n",
      "Epoch 126/300, Train Loss: 0.8072049322800758, Val Loss: 9.261713206768036\n",
      "Epoch 127/300, Train Loss: 0.8687668599379368, Val Loss: 8.905732972281319\n",
      "Epoch 128/300, Train Loss: 0.704531461535356, Val Loss: 9.19050247328622\n",
      "Epoch 129/300, Train Loss: 0.7276928757245724, Val Loss: 8.971301742962428\n",
      "Epoch 130/300, Train Loss: 0.7118036925792695, Val Loss: 8.95946741104126\n",
      "Epoch 131/300, Train Loss: 0.730310190258882, Val Loss: 9.161473418985095\n",
      "Epoch 132/300, Train Loss: 0.6794900183494275, Val Loss: 8.683152488299779\n",
      "Epoch 133/300, Train Loss: 0.7465077652381017, Val Loss: 8.972453687872205\n",
      "Epoch 134/300, Train Loss: 0.7737904929197752, Val Loss: 9.041011691093445\n",
      "Epoch 135/300, Train Loss: 0.8454432524167574, Val Loss: 8.69346627167293\n",
      "Epoch 136/300, Train Loss: 1.008785681082652, Val Loss: 9.643195816448756\n",
      "Epoch 137/300, Train Loss: 0.8229426636145665, Val Loss: 8.782456619398934\n",
      "Epoch 138/300, Train Loss: 0.7352038343747457, Val Loss: 9.082928359508514\n",
      "Epoch 139/300, Train Loss: 0.6616847291206702, Val Loss: 9.092047240052905\n",
      "Epoch 140/300, Train Loss: 0.6425559698007046, Val Loss: 8.996260600430626\n",
      "Epoch 141/300, Train Loss: 0.6259303324497663, Val Loss: 8.764377883502416\n",
      "Epoch 142/300, Train Loss: 0.6236114122928718, Val Loss: 9.426814113344465\n",
      "Epoch 143/300, Train Loss: 0.7247773852103796, Val Loss: 9.120746527399335\n",
      "Epoch 144/300, Train Loss: 0.8892146376463083, Val Loss: 9.641429798943657\n",
      "Epoch 145/300, Train Loss: 0.957123305170964, Val Loss: 8.712127319404058\n",
      "Epoch 146/300, Train Loss: 1.093022512166928, Val Loss: 9.17901464019503\n",
      "Epoch 147/300, Train Loss: 0.77867277325728, Val Loss: 8.836625899587359\n",
      "Epoch 148/300, Train Loss: 0.686852904007985, Val Loss: 8.981073107038226\n",
      "Epoch 149/300, Train Loss: 0.6742965612656031, Val Loss: 8.963243229048592\n",
      "Epoch 150/300, Train Loss: 1.042736773766004, Val Loss: 8.597813606262207\n",
      "Epoch 151/300, Train Loss: 0.7005976550090007, Val Loss: 8.661459369318825\n",
      "Epoch 152/300, Train Loss: 0.7079436860787562, Val Loss: 8.751228204795293\n",
      "Epoch 153/300, Train Loss: 0.6714828073214262, Val Loss: 9.292415176119123\n",
      "Epoch 154/300, Train Loss: 0.5689917534589768, Val Loss: 9.084166961056846\n",
      "Epoch 155/300, Train Loss: 0.5386429104285363, Val Loss: 9.094290511948723\n",
      "Epoch 156/300, Train Loss: 0.9674702026905158, Val Loss: 8.980746115956988\n",
      "Epoch 157/300, Train Loss: 0.9089418217921869, Val Loss: 8.763531957353864\n",
      "Epoch 158/300, Train Loss: 0.6954175707621452, Val Loss: 9.136131686823708\n",
      "Epoch 159/300, Train Loss: 0.608504143051612, Val Loss: 8.831861989838737\n",
      "Epoch 160/300, Train Loss: 0.5568878203630447, Val Loss: 8.434593575341362\n",
      "Epoch 161/300, Train Loss: 0.6809401525136752, Val Loss: 9.280943546976362\n",
      "Epoch 162/300, Train Loss: 0.9181275784969329, Val Loss: 9.264426401683263\n",
      "Epoch 163/300, Train Loss: 0.9457170357306798, Val Loss: 9.331148513725825\n",
      "Epoch 164/300, Train Loss: 0.788407914684369, Val Loss: 9.129283819879804\n",
      "Epoch 165/300, Train Loss: 1.0236697531663455, Val Loss: 9.409253529139928\n",
      "Epoch 166/300, Train Loss: 0.8910159603143349, Val Loss: 9.59343854870115\n",
      "Epoch 167/300, Train Loss: 0.7038465197269733, Val Loss: 9.104525515011378\n",
      "Epoch 168/300, Train Loss: 0.5809197699794403, Val Loss: 9.159187640462603\n",
      "Epoch 169/300, Train Loss: 0.5083964730684574, Val Loss: 8.990143954753876\n",
      "Epoch 170/300, Train Loss: 0.4370583560222234, Val Loss: 9.1269257409232\n",
      "Epoch 171/300, Train Loss: 0.40870470672081677, Val Loss: 8.756530089037758\n",
      "Epoch 172/300, Train Loss: 0.44456239373256, Val Loss: 8.677905355181013\n",
      "Epoch 173/300, Train Loss: 0.6329828043014575, Val Loss: 9.342110003743853\n",
      "Epoch 174/300, Train Loss: 1.1292566476724086, Val Loss: 8.953345903328486\n",
      "Epoch 175/300, Train Loss: 1.3562228245612902, Val Loss: 9.657962109361376\n",
      "Epoch 176/300, Train Loss: 1.3964659990408481, Val Loss: 9.6568837421281\n",
      "Epoch 177/300, Train Loss: 0.8079193133574266, Val Loss: 9.144526651927404\n",
      "Epoch 178/300, Train Loss: 0.5202647546162972, Val Loss: 9.089631685188838\n",
      "Epoch 179/300, Train Loss: 0.4725770686681454, Val Loss: 9.212684682437352\n",
      "Epoch 180/300, Train Loss: 0.43573396442792356, Val Loss: 9.064122319221497\n",
      "Epoch 181/300, Train Loss: 0.4728203121668253, Val Loss: 9.208903874669756\n",
      "Epoch 182/300, Train Loss: 0.7292940081694187, Val Loss: 9.625934158052717\n",
      "Epoch 183/300, Train Loss: 0.8789338730848753, Val Loss: 8.866867218698774\n",
      "Epoch 184/300, Train Loss: 0.7938223550717036, Val Loss: 9.458429396152496\n",
      "Epoch 185/300, Train Loss: 0.6801836353081924, Val Loss: 9.169134429522924\n",
      "Epoch 186/300, Train Loss: 0.6945868173470864, Val Loss: 9.48332782302584\n",
      "Epoch 187/300, Train Loss: 0.6074105093112359, Val Loss: 9.004731178283691\n",
      "Epoch 188/300, Train Loss: 0.5651771398882072, Val Loss: 9.261668043477195\n",
      "Epoch 189/300, Train Loss: 0.7026251188455483, Val Loss: 9.380601423127311\n",
      "Epoch 190/300, Train Loss: 0.7018531539501288, Val Loss: 9.106920787266322\n",
      "Epoch 191/300, Train Loss: 0.6687924048075309, Val Loss: 9.1234187909535\n",
      "Epoch 192/300, Train Loss: 0.676200918127329, Val Loss: 8.945706163133893\n",
      "Epoch 193/300, Train Loss: 0.678128449122111, Val Loss: 9.022402984755379\n",
      "Epoch 194/300, Train Loss: 0.6705416334745211, Val Loss: 8.947868474892207\n",
      "Epoch 195/300, Train Loss: 0.6959404571698262, Val Loss: 9.685321518353053\n",
      "Epoch 196/300, Train Loss: 0.750128006705871, Val Loss: 8.965611951691765\n",
      "Epoch 197/300, Train Loss: 0.6247037212006175, Val Loss: 9.43806638462203\n",
      "Epoch 198/300, Train Loss: 0.5521376870381527, Val Loss: 8.652406603097916\n",
      "Epoch 199/300, Train Loss: 0.6925186551534213, Val Loss: 8.680348873138428\n",
      "Epoch 200/300, Train Loss: 0.6924213448396096, Val Loss: 9.646090217999049\n",
      "Epoch 201/300, Train Loss: 0.7286261764856485, Val Loss: 9.636822087424141\n",
      "Epoch 202/300, Train Loss: 0.7120604063455875, Val Loss: 9.328742802143097\n",
      "Epoch 203/300, Train Loss: 0.6624961533607581, Val Loss: 9.295535045010704\n",
      "Epoch 204/300, Train Loss: 0.5751694006033433, Val Loss: 9.070328567709241\n",
      "Epoch 205/300, Train Loss: 0.6347838248962011, Val Loss: 9.016361032213483\n",
      "Epoch 206/300, Train Loss: 0.7196086917168055, Val Loss: 9.231701476233345\n",
      "Epoch 207/300, Train Loss: 0.7061018908635164, Val Loss: 9.186917517866407\n",
      "Epoch 208/300, Train Loss: 0.945470374975449, Val Loss: 9.395065256527491\n",
      "Epoch 209/300, Train Loss: 0.8818528100466116, Val Loss: 9.511488539831978\n",
      "Epoch 210/300, Train Loss: 0.8114912506861565, Val Loss: 9.555513731070928\n",
      "Epoch 211/300, Train Loss: 0.7019542941680321, Val Loss: 9.281099200248718\n",
      "Epoch 212/300, Train Loss: 0.5450595362064166, Val Loss: 8.985145245279584\n",
      "Epoch 213/300, Train Loss: 0.48875271601554676, Val Loss: 8.949050724506378\n",
      "Epoch 214/300, Train Loss: 0.47868725879070084, Val Loss: 8.823595132146563\n",
      "Epoch 215/300, Train Loss: 0.7737976499092885, Val Loss: 9.199159128325325\n",
      "Epoch 216/300, Train Loss: 0.7893299189897683, Val Loss: 9.211206248828343\n",
      "Epoch 217/300, Train Loss: 0.7860958655675252, Val Loss: 9.066233336925507\n",
      "Epoch 218/300, Train Loss: 0.7272830593280303, Val Loss: 9.753633626869746\n",
      "Epoch 219/300, Train Loss: 0.5939542198792482, Val Loss: 9.236793194498334\n",
      "Epoch 220/300, Train Loss: 0.6117390498136863, Val Loss: 9.206341168710164\n",
      "Epoch 221/300, Train Loss: 0.5781063362573966, Val Loss: 8.969784115042005\n",
      "Epoch 222/300, Train Loss: 0.5878918242760194, Val Loss: 8.657480086599078\n",
      "Epoch 223/300, Train Loss: 0.7023795662017969, Val Loss: 8.879223840577263\n",
      "Epoch 224/300, Train Loss: 1.257885172428229, Val Loss: 9.220023589474815\n",
      "Epoch 225/300, Train Loss: 1.5599886007798023, Val Loss: 9.10374731676919\n",
      "Epoch 226/300, Train Loss: 0.896473169632447, Val Loss: 8.819516846111842\n",
      "Epoch 227/300, Train Loss: 0.7858069937962752, Val Loss: 9.451262874262673\n",
      "Epoch 228/300, Train Loss: 0.4826999781987606, Val Loss: 9.283400075776237\n",
      "Epoch 229/300, Train Loss: 0.40914263732922385, Val Loss: 9.060655065945216\n",
      "Epoch 230/300, Train Loss: 0.3785952558120092, Val Loss: 9.296011115823474\n",
      "Epoch 231/300, Train Loss: 0.3967174902940408, Val Loss: 9.062581266675677\n",
      "Epoch 232/300, Train Loss: 0.46087538913274423, Val Loss: 9.047696215765816\n",
      "Epoch 233/300, Train Loss: 0.45972699805712086, Val Loss: 9.668963083199092\n",
      "Epoch 234/300, Train Loss: 0.47524258547868486, Val Loss: 9.279688341276985\n",
      "Epoch 235/300, Train Loss: 0.8851952774402423, Val Loss: 9.496920262064252\n",
      "Epoch 236/300, Train Loss: 0.9858839319302486, Val Loss: 9.692453052316393\n",
      "Epoch 237/300, Train Loss: 0.8919164727895688, Val Loss: 9.334136622292656\n",
      "Epoch 238/300, Train Loss: 0.7306871640376555, Val Loss: 9.042022722108024\n",
      "Epoch 239/300, Train Loss: 0.6893598412856078, Val Loss: 9.267360533986773\n",
      "Epoch 240/300, Train Loss: 0.6213071202620482, Val Loss: 9.168147001947675\n",
      "Epoch 241/300, Train Loss: 0.488628252194478, Val Loss: 9.062343435628074\n",
      "Epoch 242/300, Train Loss: 0.41914419058041696, Val Loss: 9.396818612303052\n",
      "Epoch 243/300, Train Loss: 0.4410898830646124, Val Loss: 8.960903712681361\n",
      "Epoch 244/300, Train Loss: 0.4555386456159445, Val Loss: 9.251845487526484\n",
      "Epoch 245/300, Train Loss: 0.5344084656773469, Val Loss: 9.31717210156577\n",
      "Epoch 246/300, Train Loss: 0.661098521680404, Val Loss: 9.278089599949974\n",
      "Epoch 247/300, Train Loss: 0.9911590962837904, Val Loss: 9.434606909751892\n",
      "Epoch 248/300, Train Loss: 1.2765069267688653, Val Loss: 9.43421995639801\n",
      "Epoch 249/300, Train Loss: 1.2426775203301357, Val Loss: 9.7310237629073\n",
      "Epoch 250/300, Train Loss: 0.8651083175952617, Val Loss: 8.628744661808014\n",
      "Epoch 251/300, Train Loss: 0.5482435290630047, Val Loss: 9.049102195671626\n",
      "Epoch 252/300, Train Loss: 0.39039771041044824, Val Loss: 9.039049787180764\n",
      "Epoch 253/300, Train Loss: 0.3104984589876273, Val Loss: 8.779888783182416\n",
      "Epoch 254/300, Train Loss: 0.30527191563294487, Val Loss: 8.82022339957101\n",
      "Epoch 255/300, Train Loss: 0.3202562225170625, Val Loss: 9.060443111828395\n",
      "Epoch 256/300, Train Loss: 0.3740705614670729, Val Loss: 8.75600962979453\n",
      "Epoch 257/300, Train Loss: 0.5134796998439691, Val Loss: 9.698988437652588\n",
      "Epoch 258/300, Train Loss: 1.1870443402192532, Val Loss: 9.046896151133947\n",
      "Epoch 259/300, Train Loss: 1.6686495970457027, Val Loss: 9.220963597297668\n",
      "Epoch 260/300, Train Loss: 1.4940011015305152, Val Loss: 9.40968005146299\n",
      "Epoch 261/300, Train Loss: 1.3495119674083513, Val Loss: 9.264746018818446\n",
      "Epoch 262/300, Train Loss: 0.6727300998492118, Val Loss: 9.08706179686955\n",
      "Epoch 263/300, Train Loss: 0.42912771889032464, Val Loss: 9.338950557368142\n",
      "Epoch 264/300, Train Loss: 0.37555502408590075, Val Loss: 9.08121396814074\n",
      "Epoch 265/300, Train Loss: 0.3411650346639829, Val Loss: 8.934675684997014\n",
      "Epoch 266/300, Train Loss: 0.32584682546364957, Val Loss: 8.98959309714181\n",
      "Epoch 267/300, Train Loss: 0.31443722993135453, Val Loss: 9.120536174092974\n",
      "Epoch 268/300, Train Loss: 0.3884245868676748, Val Loss: 8.76091434274401\n",
      "Epoch 269/300, Train Loss: 0.5791892223633253, Val Loss: 9.015916100570134\n",
      "Epoch 270/300, Train Loss: 1.275885455424969, Val Loss: 10.293097598212105\n",
      "Epoch 271/300, Train Loss: 1.811770811294898, Val Loss: 10.199680021830968\n",
      "Epoch 272/300, Train Loss: 1.0506821537629152, Val Loss: 8.629692077636719\n",
      "Epoch 273/300, Train Loss: 0.71078208226424, Val Loss: 8.749755246298653\n",
      "Epoch 274/300, Train Loss: 0.4580690927612476, Val Loss: 8.79497915506363\n",
      "Epoch 275/300, Train Loss: 0.33796200163853474, Val Loss: 8.788097390106746\n",
      "Epoch 276/300, Train Loss: 0.3077848853973242, Val Loss: 8.76997150693621\n",
      "Epoch 277/300, Train Loss: 0.3291815854035891, Val Loss: 8.874436267784663\n",
      "Epoch 278/300, Train Loss: 0.3640721927086512, Val Loss: 8.570213198661804\n",
      "Epoch 279/300, Train Loss: 0.39847010389352455, Val Loss: 8.841273094926562\n",
      "Epoch 280/300, Train Loss: 0.4655747243227103, Val Loss: 8.857617488929204\n",
      "Epoch 281/300, Train Loss: 0.6861099179738607, Val Loss: 8.924369011606489\n",
      "Epoch 282/300, Train Loss: 1.0118386079103519, Val Loss: 8.829168626240321\n",
      "Epoch 283/300, Train Loss: 1.2510723857543407, Val Loss: 9.095818860190255\n",
      "Epoch 284/300, Train Loss: 1.007222716242839, Val Loss: 9.27103294645037\n",
      "Epoch 285/300, Train Loss: 0.8401776069249862, Val Loss: 8.778532543352672\n",
      "Epoch 286/300, Train Loss: 0.6310390807879277, Val Loss: 9.25129530259541\n",
      "Epoch 287/300, Train Loss: 0.5766390399290965, Val Loss: 8.918817383902413\n",
      "Epoch 288/300, Train Loss: 0.5027491131654153, Val Loss: 8.770033546856471\n",
      "Epoch 289/300, Train Loss: 0.392147092291942, Val Loss: 8.825999566486903\n",
      "Epoch 290/300, Train Loss: 0.3599574685860903, Val Loss: 8.723546973296575\n",
      "Epoch 291/300, Train Loss: 0.34597224646653885, Val Loss: 8.703066544873375\n",
      "Epoch 292/300, Train Loss: 0.39940358461477815, Val Loss: 8.8756976383073\n",
      "Epoch 293/300, Train Loss: 0.4784954173442645, Val Loss: 9.185574276106697\n",
      "Epoch 294/300, Train Loss: 0.8716241038762607, Val Loss: 9.186403512954712\n",
      "Epoch 295/300, Train Loss: 1.654212615887324, Val Loss: 8.950905578477043\n",
      "Epoch 296/300, Train Loss: 1.673810454056813, Val Loss: 9.21270820072719\n",
      "Epoch 297/300, Train Loss: 0.9773396126734905, Val Loss: 9.518773896353585\n",
      "Epoch 298/300, Train Loss: 0.6460580549178979, Val Loss: 8.887354033333915\n",
      "Epoch 299/300, Train Loss: 0.42846936487998716, Val Loss: 8.616002576691765\n",
      "Epoch 300/300, Train Loss: 0.3390409661409182, Val Loss: 8.72048808847155\n",
      "R^2 score of the network on the test images: -0.0036554336547851562\n",
      "Test Loss: 9.864847421646118\n"
     ]
    }
   ],
   "source": [
    "# Create a model instance\n",
    "model = MedicalResNetModel(\n",
    "    num_epochs=300,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=1e-4,\n",
    "    data_loader=data_loader,\n",
    "    #dropout_rate=0.1,\n",
    "    #pretrained=False\n",
    ")\n",
    "\n",
    "# model.load_model('MedicalResNetModel_18_6212_epoch_38.pth')\n",
    "model.train()\n",
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with the best model\n",
    "# model.load_model('MedicalResNetModel_18_4400_epoch_16.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
